{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2,os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score,precision_score,accuracy_score,roc_curve\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,TensorDataset,random_split,DataLoader,SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "from vit_pytorch import ViT\n",
    "from vit_pytorch.mobile_vit import MobileViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.device(cuda)\n",
      "torch.cuda.device_count():  4\n",
      "Tesla V100-SXM2-16GB\n",
      "Tesla V100-SXM2-16GB\n",
      "Tesla V100-SXM2-16GB\n",
      "Tesla V100-SXM2-16GB\n",
      "torch.cuda.current_device() 0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"torch.device(cuda)\")\n",
    "    print(\"torch.cuda.device_count(): \", torch.cuda.device_count())\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(torch.cuda.get_device_name())\n",
    "    print(\"torch.cuda.current_device()\", torch.cuda.current_device())\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"torch.device(cpu)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJjxv_T_DNKk"
   },
   "source": [
    "# 1. Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chip=\"All\"\n",
    "X_Ctrl = np.load(\"./Datasets/Ctrl_\"+chip+\".npy\",allow_pickle=True)[:1000]\n",
    "X_VPA = np.load(\"./Datasets/VPA_\"+chip+\".npy\",allow_pickle=True)[:1000]\n",
    "y_Ctrl = torch.zeros(len(X_Ctrl), dtype=torch.int64)\n",
    "y_VPA = torch.ones(len(X_VPA), dtype=torch.int64)\n",
    "X = np.concatenate((X_Ctrl, X_VPA), axis = 0)\n",
    "y = torch.cat((y_Ctrl, y_VPA), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qyvCoE-HDNKm"
   },
   "source": [
    "# 2. Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 290,
     "status": "ok",
     "timestamp": 1627023529885,
     "user": {
      "displayName": "Yicheng Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgziKLn_im8Zl7A_SAzLLRm66nioH7fG0xuCYpJYg=s64",
      "userId": "10487961361854797289"
     },
     "user_tz": -540
    },
    "id": "Fh058iRlDNKm"
   },
   "outputs": [],
   "source": [
    "class cell_dataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.transform = transforms.ToTensor()\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(self.x[idx]).to(torch.float), F.one_hot(self.y[idx],num_classes=2).to(torch.float)\n",
    "\n",
    "dataset = cell_dataset(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 325,
     "status": "ok",
     "timestamp": 1627023533467,
     "user": {
      "displayName": "Yicheng Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgziKLn_im8Zl7A_SAzLLRm66nioH7fG0xuCYpJYg=s64",
      "userId": "10487961361854797289"
     },
     "user_tz": -540
    },
    "id": "xK5exWXUDNKn"
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "train_size = int(len(X)*0.8)\n",
    "valid_size = len(X) - train_size\n",
    "\n",
    "train_data, valid_data = random_split(dataset=dataset, lengths=[train_size, valid_size], \n",
    "                                      generator=torch.Generator().manual_seed(42))\n",
    "dataloader_train = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "dataloader_valid = DataLoader(valid_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of train :  1600\n",
      "train_class_0:  793\n",
      "train_class_1:  807\n",
      "\n",
      "Total number of valid :  400\n",
      "valid_class_0 num :  207\n",
      "valid_class_1 num :  193\n"
     ]
    }
   ],
   "source": [
    "train_data_01 = 0\n",
    "train_size = len(train_data)\n",
    "for i in range(train_size):\n",
    "    train_data_01+=int(train_data[i][1][1].item())\n",
    "print(\"Total number of train : \", train_size)\n",
    "print(\"train_class_0: \", train_size-train_data_01)\n",
    "print(\"train_class_1: \", train_data_01)\n",
    "\n",
    "valid_data_01 = 0\n",
    "valid_size = len(valid_data)\n",
    "for i in range(valid_size):\n",
    "    valid_data_01+=int(valid_data[i][1][1].item())\n",
    "print(\"\\nTotal number of valid : \", valid_size)\n",
    "print(\"valid_class_0 num : \", valid_size-valid_data_01)\n",
    "print(\"valid_class_1 num : \", valid_data_01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-KBoEbEMDNKo"
   },
   "source": [
    "# 3. Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "VIwIc2BXDNKp",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = ViT(\n",
    "    image_size = 600,\n",
    "    patch_size = 10,\n",
    "    num_classes = 2,\n",
    "    dim = 1024,\n",
    "    depth = 1,\n",
    "    heads = 2,\n",
    "    mlp_dim = 2048,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    ").to(device)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "    \n",
    "# #Freeze model weights\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = True\n",
    "# for param in model.fc.parameters():\n",
    "#     param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "image_size = 600\n",
    "test_input = torch.ones(1,3,image_size,image_size).to(device)\n",
    "print(model(test_input).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "loss_function = nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "\n",
    "def train(model,device,dataloader_train,loss_function,optimizer):\n",
    "    losses_train = []\n",
    "    n_train = 0\n",
    "    acc_train = 0\n",
    "    optimizer.step()\n",
    "    model.train()\n",
    "    for x, y in dataloader_train:\n",
    "        n_train += y.size()[0]\n",
    "        model.zero_grad()  # 勾配の初期化\n",
    "        x = x.to(device)  # テンソルをGPUに移動\n",
    "        y = y.to(device)\n",
    "        output = model.forward(x)  # 順伝播\n",
    "        loss = loss_function(output, y)  # 誤差(クロスエントロピー誤差関数)の計算\n",
    "        loss.backward()  # 誤差の逆伝播\n",
    "        optimizer.step()  # パラメータの更新\n",
    "        acc_train += (output.argmax(1) == y[:,1]).float().sum().item()\n",
    "        losses_train.append(loss.tolist())\n",
    "    return np.mean(losses_train), (acc_train/n_train)\n",
    "        \n",
    "def valid(model,device,dataloader_valid,loss_function):\n",
    "    losses_valid = []\n",
    "    n_val = 0\n",
    "    acc_val = 0\n",
    "    model.eval()\n",
    "    for x, y in dataloader_valid:\n",
    "        n_val += y.size()[0]\n",
    "        x = x.to(device)  # テンソルをGPUに移動\n",
    "        y = y.to(device)\n",
    "        output = model.forward(x)  # 順伝播\n",
    "        loss = loss_function(output, y)  # 誤差(クロスエントロピー誤差関数)の計算\n",
    "        acc_val += (output.argmax(1) == y[:,1]).float().sum().item()\n",
    "        losses_valid.append(loss.tolist())\n",
    "    return np.mean(losses_valid), (acc_val/n_val)\n",
    "\n",
    "history = {'loss_train': [], 'loss_valid': [],'acc_train':[],'acc_valid':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/cuda/Loss.cu:129: operator(): block: [0,0,0], thread: [0,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:129: operator(): block: [0,0,0], thread: [4,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/11386037.1.gpu/ipykernel_127748/3723751137.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mloss_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataloader_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mloss_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataloader_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/11386037.1.gpu/ipykernel_127748/550181251.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, dataloader_train, loss_function, optimizer)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 順伝播\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 誤差(クロスエントロピー誤差関数)の計算\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 誤差の逆伝播\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# パラメータの更新\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0macc_train\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python7_env/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python7_env/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    loss_train, acc_train = train(model,device,dataloader_train,loss_function,optimizer)\n",
    "    loss_valid, acc_valid = valid(model,device,dataloader_valid,loss_function)\n",
    "    scheduler.step()\n",
    "    \n",
    "    history['loss_train'].append(loss_train)\n",
    "    history['loss_valid'].append(loss_valid)\n",
    "    history['acc_train'].append(acc_train)\n",
    "    history['acc_valid'].append(acc_valid) \n",
    "    print('EPOCH: {}, Train [Loss: {:.3f}, Accuracy: {:.3f}], Valid [Loss: {:.3f}, Accuracy: {:.3f}]'\n",
    "          .format(epoch, loss_train, acc_train, loss_valid, acc_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train processing plot\n",
    "n_epochs = 10\n",
    "epochs=range(1,n_epochs+1)\n",
    "plt.ylim(0,1.0)\n",
    "plt.plot(epochs, history['acc_train'], 'b', label='Training accuracy')  \n",
    "plt.plot(epochs, history['acc_valid'], 'r', label='Validation accuracy')\n",
    "plt.title('Training and Validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "X = X_test\n",
    "y = y_test\n",
    "y_pred = []\n",
    "out_pred = []\n",
    "total = X.shape[0]\n",
    "\n",
    "model.eval()\n",
    "for n in range(total):\n",
    "    img = X[n]\n",
    "    label = y[n]\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    input_tensor = transform(img).unsqueeze(0).to(device)\n",
    "    output = model(input_tensor)\n",
    "    pred = output.argmax(1).cpu().item()\n",
    "    out_pred.append(output[0][1].item())\n",
    "    y_pred.append(pred)\n",
    "\n",
    "y_pred = np.array(y_pred)\n",
    "out_pred = np.array(out_pred)\n",
    "print(\"total_test: {:}\" .format(total))\n",
    "print('accuracy_score: {:.3f}'.format(accuracy_score(y,y_pred)))\n",
    "print('precision_score: {:.3f}'.format(precision_score(y,y_pred)))\n",
    "print('roc_auc_score: {:.3f}'.format(roc_auc_score(y, out_pred)))\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y, out_pred,drop_intermediate=True)\n",
    "plt.plot(fpr, tpr, marker='.')\n",
    "plt.xlabel('FPR: False positive rate')\n",
    "plt.ylabel('TPR: True positive rate')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 99. Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "torch.save(model,\"SelfResnet18_L2F1_250.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "KIMIA_CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
