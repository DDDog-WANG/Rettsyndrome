{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2,os\n",
    "from skimage import io\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score,precision_score,accuracy_score,roc_curve\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import random_split,Dataset,DataLoader,SubsetRandomSampler\n",
    "from torch.utils.data import Dataset,TensorDataset,random_split,SubsetRandomSampler\n",
    "from torch.utils.data.dataset import Subset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.device(cuda)\n",
      "torch.cuda.device_count():  4\n",
      "Tesla V100-SXM2-16GB\n",
      "Tesla V100-SXM2-16GB\n",
      "Tesla V100-SXM2-16GB\n",
      "Tesla V100-SXM2-16GB\n",
      "torch.cuda.current_device() 0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"torch.device(cuda)\")\n",
    "    print(\"torch.cuda.device_count(): \", torch.cuda.device_count())\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(torch.cuda.get_device_name())\n",
    "    print(\"torch.cuda.current_device()\", torch.cuda.current_device())\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"torch.device(cpu)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJjxv_T_DNKk"
   },
   "source": [
    "# 1. Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "chip=\"H3K27ac\"\n",
    "X_Ctrl = np.load(\"./Datasets/Ctrl_\"+chip+\".npy\",allow_pickle=True)\n",
    "X_VPA = np.load(\"./Datasets/VPA_\"+chip+\".npy\",allow_pickle=True)\n",
    "y_Ctrl = torch.zeros(len(X_Ctrl), dtype=torch.int64)\n",
    "y_VPA = torch.ones(len(X_VPA), dtype=torch.int64)\n",
    "X = np.concatenate((X_Ctrl, X_VPA), axis = 0)\n",
    "y = torch.cat((y_Ctrl, y_VPA), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qyvCoE-HDNKm"
   },
   "source": [
    "# 2. Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "executionInfo": {
     "elapsed": 290,
     "status": "ok",
     "timestamp": 1627023529885,
     "user": {
      "displayName": "Yicheng Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgziKLn_im8Zl7A_SAzLLRm66nioH7fG0xuCYpJYg=s64",
      "userId": "10487961361854797289"
     },
     "user_tz": -540
    },
    "id": "Fh058iRlDNKm"
   },
   "outputs": [],
   "source": [
    "class cell_dataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.transform = transforms.ToTensor()\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(self.x[idx]).to(torch.float), F.one_hot(self.y[idx],num_classes=2).to(torch.float)\n",
    "\n",
    "dataset = cell_dataset(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-KBoEbEMDNKo"
   },
   "source": [
    "# 3. ResNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/groups/4/gaa50089/acd13264yb/Epigenetic/Classification/Models/Resnet10_noavg_H3K27ac.pkl\n"
     ]
    }
   ],
   "source": [
    "modelpath=\"/groups/4/gaa50089/acd13264yb/Epigenetic/Classification/Models/\"\n",
    "\n",
    "resnet=\"Resnet10_noavg\"\n",
    "weight=torch.load(modelpath+resnet+\"_\"+chip+\".pkl\")\n",
    "print(modelpath+resnet+\"_\"+chip+\".pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "xs6IXgvsDNKp",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acd13264yb/python7_env/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "if resnet==\"Resnet10_noavg\":\n",
    "    class ResNet(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(ResNet,self).__init__()\n",
    "            self.resnet = models.resnet18(weights=True)\n",
    "            self.resnet.layer3 = nn.Sequential()\n",
    "            self.resnet.layer4 = nn.Sequential()\n",
    "            self.resnet.avgpool = nn.Sequential()\n",
    "            self.resnet.fc = nn.Linear(128*75*75, 2)\n",
    "            self.resnet.load_state_dict(weight)\n",
    "        def forward(self, x):\n",
    "            x = self.resnet(x)\n",
    "            x = nn.Softmax(dim=1)(x)\n",
    "            return x\n",
    "elif resnet==\"Resnet10\":\n",
    "    class ResNet(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(ResNet,self).__init__()\n",
    "            self.resnet = models.resnet18(weights=True)\n",
    "            self.resnet.layer3 = nn.Sequential()\n",
    "            self.resnet.layer4 = nn.Sequential()\n",
    "            self.resnet.fc = nn.Linear(128, 2)\n",
    "            self.resnet.load_state_dict(weight)\n",
    "        def forward(self, x):\n",
    "            x = self.resnet(x)\n",
    "            x = nn.Softmax(dim=1)(x)\n",
    "            return x\n",
    "elif resnet==\"Resnet18\":\n",
    "    class ResNet(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(ResNet,self).__init__()\n",
    "            self.resnet = models.resnet18(weights=True)\n",
    "            self.resnet.fc = nn.Linear(512, 2)\n",
    "            self.resnet.load_state_dict(weight)\n",
    "        def forward(self, x):\n",
    "            x = self.resnet(x)\n",
    "            x = nn.Softmax(dim=1)(x)\n",
    "            return x\n",
    "    \n",
    "model = ResNet().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Define Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "loss_function = nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "\n",
    "def train(model,device,dataloader_train,loss_function,optimizer):\n",
    "    losses_train = []\n",
    "    n_train = 0\n",
    "    acc_train = 0\n",
    "    optimizer.step()\n",
    "    model.train()\n",
    "    for x, y in dataloader_train:\n",
    "        n_train += y.size()[0]\n",
    "        model.zero_grad()  # 勾配の初期化\n",
    "        x = x.to(device)  # テンソルをGPUに移動\n",
    "        y = y.to(device)\n",
    "        output = model.forward(x)  # 順伝播\n",
    "        loss = loss_function(output, y)  # 誤差(クロスエントロピー誤差関数)の計算\n",
    "        loss.backward()  # 誤差の逆伝播\n",
    "        optimizer.step()  # パラメータの更新\n",
    "        acc_train += (output.argmax(1) == y[:,1]).float().sum().item()\n",
    "        losses_train.append(loss.tolist())\n",
    "    return np.mean(losses_train), (acc_train/n_train)\n",
    "        \n",
    "def valid(model,device,dataloader_valid,loss_function):\n",
    "    losses_valid = []\n",
    "    n_val = 0\n",
    "    acc_val = 0\n",
    "    model.eval()\n",
    "    for x, y in dataloader_valid:\n",
    "        n_val += y.size()[0]\n",
    "        x = x.to(device)  # テンソルをGPUに移動\n",
    "        y = y.to(device)\n",
    "        output = model.forward(x)  # 順伝播\n",
    "        loss = loss_function(output, y)  # 誤差(クロスエントロピー誤差関数)の計算\n",
    "        acc_val += (output.argmax(1) == y[:,1]).float().sum().item()\n",
    "        losses_valid.append(loss.tolist())\n",
    "    return np.mean(losses_valid), (acc_val/n_val)\n",
    "\n",
    "history = {'loss_train': [], 'loss_valid': [],'acc_train':[],'acc_valid':[]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Train by KFold of Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "loss_function = nn.BCELoss()\n",
    "\n",
    "splits=KFold(n_splits=5,shuffle=True,random_state=42)\n",
    "batch_size = 128\n",
    "n_epochs = 1\n",
    "    \n",
    "for fold, (train_idx, val_idx) in enumerate(splits.split(np.arange(len(dataset)))):\n",
    "    print('Fold {}'.format(fold + 1))\n",
    "    \n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(val_idx)\n",
    "    dataloader_train = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "    dataloader_valid = DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler)\n",
    "\n",
    "    model = ResNet().to(device)\n",
    "    ngpu = 4\n",
    "    if (device.type == 'cuda') and (ngpu > 1):\n",
    "        model = nn.DataParallel(model, list(range(ngpu)))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        loss_train, acc_train = train(model,device,dataloader_train,loss_function,optimizer)\n",
    "        loss_valid, acc_valid = valid(model,device,dataloader_valid,loss_function)\n",
    "        scheduler.step()\n",
    "        print('EPOCH: {}, Train [Loss: {:.3f}, Accuracy: {:.3f}], Valid [Loss: {:.3f}, Accuracy: {:.3f}]'\n",
    "              .format(epoch, loss_train, acc_train, loss_valid, acc_valid))\n",
    "        \n",
    "        history['loss_train'].append(loss_train)\n",
    "        history['loss_valid'].append(loss_valid)\n",
    "        history['acc_train'].append(acc_train)\n",
    "        history['acc_valid'].append(acc_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss_train_avg = np.zeros(n_epochs)\n",
    "loss_valid_avg = np.zeros(n_epochs)\n",
    "acc_train_avg = np.zeros(n_epochs)\n",
    "acc_valid_avg = np.zeros(n_epochs)\n",
    "for num in range(5):\n",
    "    loss_train = history['loss_train'][num*n_epochs:(num+1)*n_epochs]\n",
    "    loss_valid = history['loss_valid'][num*n_epochs:(num+1)*n_epochs]\n",
    "    acc_train = history['acc_train'][num*n_epochs:(num+1)*n_epochs]\n",
    "    acc_valid = history['acc_valid'][num*n_epochs:(num+1)*n_epochs]\n",
    "    loss_train_avg+=loss_train\n",
    "    loss_valid_avg+=loss_valid\n",
    "    acc_train_avg+=acc_train\n",
    "    acc_valid_avg+=acc_valid\n",
    "    \n",
    "    plt.ylim(0,1.0)\n",
    "    plt.plot(range(1,n_epochs+1), acc_train, 'b', label='Training accuracy')  \n",
    "    plt.plot(range(1,n_epochs+1), acc_valid, 'r', label='Validation accuracy')\n",
    "    plt.title(\"Fold \"+str(num)+' Training and Validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.figure()\n",
    "    plt.show()\n",
    "    \n",
    "loss_train_avg = loss_train_avg/5\n",
    "loss_valid_avg = loss_valid_avg/5\n",
    "acc_train_avg = acc_train_avg/5\n",
    "acc_valid_avg = acc_valid_avg/5\n",
    "plt.ylim(0,1.0)\n",
    "plt.plot(range(1,n_epochs+1), acc_train_avg, 'b', label='Training accuracy')  \n",
    "plt.plot(range(1,n_epochs+1), acc_valid_avg, 'r', label='Validation accuracy')\n",
    "plt.title(' Average Training and Validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. valid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_test: 451\n",
      "accuracy_score: 0.996\n",
      "precision_score: 1.000\n",
      "roc_auc_score: 1.000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "out_pred = []\n",
    "total = 0\n",
    "\n",
    "model.eval()\n",
    "for x, y in dataloader_valid:\n",
    "    total += y.size()[0]\n",
    "    x = x.to(device)  # テンソルをGPUに移動\n",
    "    y = y.to(device)\n",
    "    output = model.forward(x)  # 順伝播\n",
    "    for i in range(y.size()[0]):\n",
    "        out_pred.append(output[i][1].item())\n",
    "        y_pred.append(output.argmax(1)[i].item())\n",
    "        y_true.append(y[i][1].item())\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "out_pred = np.array(out_pred) \n",
    "print(chip)\n",
    "print(resnet)\n",
    "print('accuracy_score: {:.3f}'.format(accuracy_score(y_true,y_pred)))\n",
    "print('precision_score: {:.3f}'.format(precision_score(y_true,y_pred)))\n",
    "print('roc_auc_score: {:.3f}'.format(roc_auc_score(y_true, out_pred)))\n",
    "print(\"total_test: {:}\" .format(total))\n",
    "fpr, tpr, thresholds = roc_curve(y_true, out_pred,drop_intermediate=True)\n",
    "plt.plot(fpr, tpr, marker='.')\n",
    "plt.xlabel('FPR: False positive rate')\n",
    "plt.ylabel('TPR: True positive rate')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 99. Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "torch.save(model.module.resnet.state_dict(),\"Models/Resnet17C1F.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fc = nn.Sequential(\n",
    "#     nn.Linear(128, 512),\n",
    "#     nn.Dropout(p=0.5),\n",
    "#     nn.ReLU(inplace=True),\n",
    "#     nn.Linear(512, 256),\n",
    "#     nn.ReLU(inplace=True),\n",
    "#     nn.Linear(256, 2)\n",
    "# )\n",
    "\n",
    "# #Freeze model weights\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = True\n",
    "# for param in model.fc.parameters():\n",
    "#     param.requires_grad = True"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "KIMIA_CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
